{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_JkO5QDGg9Y",
        "outputId": "937aa7eb-5151-4654-947c-d0ee913deddd"
      },
      "outputs": [],
      "source": [
        "%pip install -Uq llama-index\n",
        "\n",
        "# -U (Upgrade): Tells pip to upgrade the package to the latest version if it's already installed.\n",
        "# -q (Quiet): Suppresses most of the output messages (like installation progress and dependency checks), making the output cleaner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KewnL3tGtaC",
        "outputId": "8fd75d28-eedc-4f6e-c74f-36d93d3cb41c"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = getpass.getpass(\"OpenAI API Key: \")\n",
        "\n",
        "# os.environ is a dictionary-like object in the os module; it co ntains the environment variables of your operating system (e.g., PATH, HOME, OPENAI_API_KEY, etc.)\n",
        "# prompt the user for input without showing what they type (nothing appears on screen);\task for passwords or API keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C86ImsifHPg4"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "JRh3EvB2B9Vn",
        "outputId": "24201a39-39ba-4914-954c-0fa4994ec6ee"
      },
      "outputs": [],
      "source": [
        "documents = SimpleDirectoryReader(\"data\").load_data()\n",
        "\n",
        "# “PDF / HTML / CSV / DOCX → data connectors → documents”\n",
        "\n",
        "# - loads documents from the \"data\" directory. Internally:\n",
        "# - uses a Data Connector to read each file\n",
        "# - each file becomes a Document object that can be later split into Nodes (text chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z26rwZwsHiLE"
      },
      "outputs": [],
      "source": [
        "index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "# “document → node → embeddings → index”\n",
        "\n",
        "# - converts the documents into nodes (chunks of text)\n",
        "# - generates embeddings (via Hugging Face or OpenAI)\n",
        "# - stored in a vector index to support semantic search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trkHiCvWHlza"
      },
      "outputs": [],
      "source": [
        "query_engine = index.as_query_engine()\n",
        "\n",
        "# “query → router → retrievers → index”\n",
        "\n",
        "# - This sets up the retrieval pipeline for querying the index.\n",
        "# - It wraps the index in a QueryEngine that handles:\n",
        "#   * Receiving the user query\n",
        "#   * Routing it to one or more retrievers\n",
        "#   * Returning top relevant nodes for synthesis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "I-SnzNRKHmod",
        "outputId": "090daee9-a3cb-4c90-e959-6db21ec7c206"
      },
      "outputs": [],
      "source": [
        "response = query_engine.query(\"What is the first article?\")\n",
        "\n",
        "# “query → retriever(s) → index → Response Synthesizer → LLM → response”\n",
        "\n",
        "# - Ask a natural language question against the indexed documents\n",
        "# - The query is embedded and compared to the stored document embeddings to retrieve the most relevant content\n",
        "# - Relevant content is fetched and passed into the Response Synthesizer\n",
        "# - the LLM is used to generate a human-readable answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxwnJDWPHnWC"
      },
      "outputs": [],
      "source": [
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
